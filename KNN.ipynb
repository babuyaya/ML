{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c763f257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from bayes_opt import BayesianOptimization\n",
    "import xgboost as xgb\n",
    "\n",
    "data = pd.read_excel('3.xlsx')\n",
    "x = data.iloc[:, :2].values \n",
    "y = data.iloc[:, -1].values  \n",
    "\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "\n",
    "def optimize(max_depth, learning_rate, n_estimators, subsample, colsample_bytree):\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    r2_scores = []\n",
    "    for train_idx, val_idx in kfold.split(x):\n",
    "        train_x, val_x = x[train_idx], x[val_idx]\n",
    "        train_y, val_y = y[train_idx], y[val_idx]\n",
    "        \n",
    "        model = xgb.XGBRegressor(\n",
    "            max_depth=int(max_depth),\n",
    "            learning_rate=learning_rate,\n",
    "            n_estimators=int(n_estimators),\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            objective='reg:squarederror',\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(train_x, train_y)\n",
    "        predictions = model.predict(val_x)\n",
    "        r2 = r2_score(val_y, predictions)\n",
    "        r2_scores.append(r2)\n",
    "    return np.mean(r2_scores)\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=optimize,\n",
    "    pbounds={\n",
    "        'max_depth': (3, 10),\n",
    "        'learning_rate': (0.01, 0.3),\n",
    "        'n_estimators': (50, 300),\n",
    "        'subsample': (0.6, 1.0),\n",
    "        'colsample_bytree': (0.6, 1.0)\n",
    "    },\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "optimizer.maximize(n_iter=100000, init_points=10)\n",
    "\n",
    "# 提取最佳参数\n",
    "best_params = optimizer.max['params']\n",
    "best_params['max_depth'] = int(best_params['max_depth'])\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "\n",
    "best_model = xgb.XGBRegressor(\n",
    "    **best_params,\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42\n",
    ")\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "best_model.fit(train_x, train_y)\n",
    "train_pred = best_model.predict(train_x)\n",
    "test_pred = best_model.predict(test_x)\n",
    "\n",
    "# 计算训练集的评估指标\n",
    "train_mse = mean_squared_error(train_y, train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(train_y, train_pred)\n",
    "train_r2 = r2_score(train_y, train_pred)\n",
    "\n",
    "# 计算测试集的评估指标\n",
    "test_mse = mean_squared_error(test_y, test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(test_y, test_pred)\n",
    "test_r2 = r2_score(test_y, test_pred)\n",
    "\n",
    "# 输出结果\n",
    "print(f'Train - MSE: {train_mse}, RMSE: {train_rmse}, MAE: {train_mae}, R^2 score: {train_r2}')\n",
    "print(f'Test - MSE: {test_mse}, RMSE: {test_rmse}, MAE: {test_mae}, R^2 score: {test_r2}')\n",
    "print(f'Best parameters found: {best_params}')\n",
    "print(f'Best R^2 score during optimization: {optimizer.max[\"target\"]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
